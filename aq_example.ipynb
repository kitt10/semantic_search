{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kitt10/semantic_search/blob/main/aq_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Asking Questions Model - Example Usage"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Download model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Downlaod the AQ model from the GDrive\n",
        "!gdown 1PSaBDLs5z6OF2ej8O_JemzVxNwSaHiic && unzip -u aq.zip\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Install libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers=4.18"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Initialize the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikKpBRH-qljh"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from random import choice\n",
        "\n",
        "\n",
        "class AQModel:\n",
        "    \"\"\" Asking Questions Model \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.tokenizer = None\n",
        "        self.load_tokenizer()\n",
        "        self.model = None\n",
        "        self.load_model()\n",
        "        \n",
        "    def load_tokenizer(self):\n",
        "        self.tokenizer = T5Tokenizer('aq/tokenizer/sentencepiece.model')\n",
        "        print(f'Tokenizer loaded.')\n",
        "\n",
        "    def load_model(self):\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained('aq/model', from_tf=True)\n",
        "        print(f'AQ-Model loaded.')\n",
        "\n",
        "    def generate(self, context, choose_from=1):\n",
        "        input_ids = self.tokenizer([context], return_tensors=\"pt\").input_ids\n",
        "        outputs = self.model.generate(input_ids, \n",
        "                                        max_length=128,\n",
        "                                        num_beams=5,\n",
        "                                        no_repeat_ngram_size=2, \n",
        "                                        num_return_sequences=choose_from, \n",
        "                                        early_stopping=True)\n",
        "\n",
        "        decoded = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        \n",
        "        if choose_from > 1:\n",
        "            print(f'Randomly choosing from: ')\n",
        "            for d in decoded:\n",
        "                print(f'{self.parse(d)}')\n",
        "        \n",
        "        return self.parse(choice(decoded))\n",
        "    \n",
        "    def parse(self, generated):\n",
        "        try:\n",
        "            q, a = generated.split('Answer:')\n",
        "            return q.replace('Question:', '').split('?')[0].strip()+'?', a.strip()\n",
        "        except:\n",
        "            print('W: Not parsed.')\n",
        "            return generated\n",
        "        \n",
        "\n",
        "aq = AQModel()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Example usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WcHqMGt-qljj",
        "outputId": "6bb11ba1-dc2f-48a9-bebb-8ae921376f0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: How long did Joe Franklin's program run on local television?\n",
            "A: 43 years\n"
          ]
        }
      ],
      "source": [
        "context = \"\"\"\n",
        "    His name is Joe Franklin, and his program ran for 43 years on local \n",
        "    television in New York. And he claims that he invented the talk show format.\n",
        "\"\"\"\n",
        "\n",
        "question, answer = aq.generate(context, choose_from=1)\n",
        "\n",
        "print(f'Q: {question}')\n",
        "print(f'A: {answer}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "trans",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
